1 --> FCN		-->	40/33
2 --> COSA		--> 60/48
3 --> ITIM/DO	--> 40/32
4 --> NDC/CA	-->	40/31
5 --> PKI/CF	-->	40/31
6 --> SC		-->	60/49


ITIM and Devop

ITIL(Information Technology Infrastructure Library) --> Framework in 1980s

CCTA (United Kingdom's Central Computer and Telecommunications Agency) released ITIL v1 --> 1989
CCTA folded  into the Office of Government Commerce and released ITIL v2 --> 2001
ITIL v3 emerged in 2007 and was updated in 2011

Axelos currently oversees ITIL development
U.K. Cabinet Office and Capita PLC formed Axelos in 2013
It announced the latest ITIL guidelines in 2017, releasing ITIL v4 and related modules throughout 
2019 and 2020

this is made to standartized the	
	1. Selection
	2. Planning
	3. delivery
	4. maintenance 
	5. overall life cycle of IT service within bussiness
	
Goal --> efficiency and predictable service delivery

ITIL framework IT administrators to be business service partners, rather than just back-end support

ITIL guidelines and best practices align to IT department actions and expenses to business needs and 
change them as the business grows or shifts direction


## Best Practice Approaches :

	1.  Proprietary knowledge/internal experience
	2. 	Standards/industry practices
	3.	Training and education/academic research

## Why Is ITIL So Successful?

	1. Vendor Neutrality
	2. Nonprescriptive --> ITIL has recommended the approach of “adopt and adapt” 
	3. Best Practice --> delivers the knowledge and guidance from the best sources of service management practices across the world


## Type of Services

	1. Core Services		--> Mandatory
	2. Enabling Services	--> Mandatory
	3. Enhancing Services	--> Optional

## Customer

	1. Internal Customer --> work in the same organization as the service provider
	2. External Customer --> not employed by the organization or are employed by a separate legal entity 
	

## Stakeholder
	
	1. Users		--> individuals or groups that use the service on a day-to-day basis
	2. Suppliers	--> classed as third parties who have responsibility for the supply of goods or services
	
	
IT Service Management
	
	1. Service Management 	--> A set of specialized organizational capabilities for providing value to customers in the form of services
	2. Service provider	  	--> An organization supplying services to one or more internal or external customers
	3. IT service management 
							--> implementation and management of quality IT services that meet the needs of the business
							--> ITSM recommends that this relationship and the service requirements of business need, cost, and performance 
							are	documented in a service level agreement (SLA)
	4. IT service provider	--> A service provider that provides IT services to internal or external customer
	
## IT Service Provider Types
	
	1. Type I: Internal Service Provider	--> several Type I service providers within a single organization
	2. Type II: Shared Services Unit		--> provides shared IT services to more than one business unit
	3. Type III: External Service Provider	--> provides IT services to external customers
	
## Service Lifecycle

	1. Service Stratergy	--> covers the core of the lifecycle, setting the strategic approach for service management activities	
	2. Service Design		--> guidance on the design and development of services according to the requirements of customer
	3. Service Transition	--> guidance on transition of new or changed services into live environment,development and improvement of capabilities
	4. Service Operation	--> Covers management of the day-to-day delivery of services,optimizing effectiveness and efficiency
	5. Continual Service Improvement --> s guidance on maintenance of value creation and continual alignment to changing business needs
	
###########################################################################################################################################

### Service Stratergy

	1. core stage of the service lifecycle
	2. purpose is to define the strategic approach for service management across the whole lifecycle

Objective : 
			
			1. Identifying the services and the customers who use them
			2. Defining the processes and services that will deliver the strategic plans for IT service management 
and the level of investment that will be required
			3.
			
Setting Scope --> what will be delivered
			
			1. Two aspects are covered by the guidance in service strategy
					- Defining a strategy that gives a service provider guidance and
					- recommendations about delivering services to meet a customer’s business outcomes
					- Defining a strategy for managing those services
					
			
			
Assets :
	- However the assets are perceived, the performances of all assets are part of defining the value of a 
service
	
ITIL provides us with these definitions for assets:
				1. Asset: Any resource or capability
				2. Customer asset: Any resource or capability used by a customer to achieve a business outcome
				3. Service asset: Any resource or capability used by a service provider to deliver services to a customer

There are two types of asset used by both service providers and customers:
				- resources and capabilities. They are used to create value in the form of goods and services		
				

Capabilities : 
				- Capabilities are the assets that represent the organization’s ability to do something to achieve 
value, such as the ability to coordinate, control, and deploy resources in the form of a service
				
				- Capability --> Management --> organization --> processes --> knowledge --> People (experience,skill,relationship)
				- Resources --> Financial Capital --> Infrastructure --> Applications --> information --> People (no. of employees)
	
## Risk 

# Management of Risk
	
	- Service Strategy publication does not provide a comprehensive organization-wide approach but considers the basic
	risk management that is required by a project implementing an IT service management approach
	- The corporate approach to risk identification, assessment, management, and mitigation must be enhanced by the
	IT service provider, not be in conflict with it

# Identifying Risks
	- This is where the risk is identified and named, which does not mean you have to immediately explain or quantify the risk,
	but it should be possible to capture what may impact the project or strategy that is under consideration
	- Each identified or suspected risk should be documented, and the potential consequences of the risk captured should 
	be part of the record
	- A commonly used mechanism for capture is a risk register
	- It will be managed through the continual service improvement lifecycle stage
	
# Analysis od Risk 

	- Analysis
			1. Quantitative --> based on quantities(numeric)
			2. Qualitative --> based on attributes/behavirous

# Risk Management
	
	- Risk management has its basis in probability approaches and theories and can be an extremely mathematical discipline
	- Risk is the likelihood or potential for something to happen
	- It is important to ensure that common sense is also applied 
	- A risk that has been identified with 100 percent probability is not a risk; it is a certainty
	- This should be classified as an issue and be recorded and managed accordingly
	- Similarly, a risk with a 0 percent probability can be removed from the risk register
	
	
	
## Service Strategy Process

There are five processes within the service strategy lifecycle stage:
		1. Strategy management for IT services
		2. Service portfolio management
		3. IT financial management
		4. Demand management
		5. Business relationship management
		
1. Strategy Management for IT service
	- First processes under ITIL service strategy
	- During this stage, you will assess, define, and execute strategies for your service offerings
	1. Assessment --> Evaluting market position of the bussiness(impact of your service/ competitors offerings)
	2. Definition (Scope) -->  as well as identify and recommend services for different customer segments 
	3. Execution --> The final step is implementation.
	
	
2. IT Financial Management
	- Focuses on service valuation
	- This process involves accounting, budgeting, and charging services so that the organization covers cost and generates
	profits for those services
	- There steps are known as the "ABC" of Financial Management for IT Services
		1. Accounting --> understand exactly what you’re spending on IT services
		2. Budgeting  -->  accurate budget is crucial for delivering IT services effectively and consistently
				- typically occurs once a year with regular monthly monitoring
				- budget oversees three main IT spending categories:
						1. Capital expenditures
						2. Operational costs
						3. Strategic investments
		3. Charging --> Charging covers the process for billing customers based on the services they use, which involves developing 
rates and a chargeback system

3. Service portfolio management :
	
	- monitors your services in the pipeline from start to finish
	- monitoring your services end-to-end, you can more effectively justify service needs based on 
concrete business value
	- A service portfolio has three parts:
			1. Service catalog --> list --> currently
			2. Service pipeline --> list --> future
			3. Retired service catalog --> expired / deprecated
	
	- The service catalog is an overview of all the services you currently offer to customers
	- Your service pipeline contains any services that are not yet visible to the customer
	- SPM follows each service through the pipeline from funding, design, and development to testing and deployment.

There are four steps to basic Service portfolio Management:
			1. Define desired outcomes for a proposed service
			2. Analyze the impact this new or changed service will have on your other services in the portfolio
			3. Approve a new service (or change) with a formal proposal and initiate the design stage following authorization\
			4. Charter services, communicate decisions, and allocate resources for successful service deployment


4. Demand management : 
		
			- Demand management helps businesses understand, predict, and influence customer demand for their IT services
			- Demand management typically involves three primary activities:
					1. Analyzing 
							- Analyze current customer use of services by tracking service desk data
							- These data are called Patterns of Business Activity
							- You can use PBA to measure components of customer service usage like:
										1.Frequency
										2.Volume
										3.Duration
										4.Location
					2. Anticipating 
							- Communicate with your customers about their forecasted needs, track trends and rely on your data analysis
					3. Influencing
							- Businesses sometimes need to influence customer service consumption to mitigate risks and expenses
							- For example, if a customer exceeds their expected service usage, this can add significant costs for the 
							business to meet that demand 
							- You can influence demand  through financial or technical means, such as network throttling or charging fees 
							for exceeding usage limits
					
5. Business relationship management : 

			- Focuses on developing strong client relationships
			- There are several processes for executing a successful BRM program
					1. Maintain customer relationship
					2. Identifying service requirements
					3. Acquire new customers
					4. Solicit customer satisfaction feedback
					5. Handle complaints
					6. Monitor compaints and incidents


###########################################################################################################################################

### Service Design

	- Service design is involved in planning both new services and changes to existing services
	
Scope
	- ITIL service design considers not only the current requirement; it extrapolates from this to identify possible future
	needs and ensures that the service being designed will be able to be developed to meet these requirements too
	- It ensures that the design fits the requirements, and tries to take advantage of technical developments to deliver 
	an innovative service
	
- processes included within service design are as follows
			1.Design coordination
			2.Service catalog management
			3.Service level management
			4.Availability management and Capacity management
			5.IT service continuity management 
			6.Information security management 
			7.Supplier management
			
#Describing the service
service design package		

		- ITIL recommends producing key output from the service design stage: service design package (SDP)
		- service design package consists of one or more documents, produced during the service design stage,
		that describe all aspects of the service, throughout its lifecycle
		- Typical contents of an SDP include the following:
					1. Original agreed business requirements for the service 
					2. How the service will be used
					3. Key contacts and stakeholders
					4. Functional requirements
					5. Management requirements
					6. Service level requirements
					7. Technical design of the new or changed service including hardware, software, networks, environments, data, 
					applications, technology, tools, and documentation
					8. Sourcing strategy
					9. New or changed processes required to support the service
					10.Organizational readiness assessment
					
Key Element of Service Design
	
	- Service design, as described within the ITIL framework, takes a holistic view of what is required to design and deliver a service
	
	- Successful service design must therefore consider the four key elements, sometimes known as the 
	four p’s:
				1. People -->  best technical design will fail if the people who need to use it or support it are not adequately 
				prepared
				2. Processes -->  the new service may require additional processes to be designed, such as an authorization or
				procurement process 
				3. Products --> Products are not only the services that result from the service design stage itself but also the 
				technology and tools that are chosen to assist in the design or to support the service later
				4. Partners --> Partners are those specialist suppliers—usually external third-party suppliers, manufacturers, and 
				vendors—that provide part of the overall service

Aspect of Service Design
	
	- Concentrating on just the service solution will not be sufficient; other aspects need to be considered
	- There are five aspects of service design :
					1. Service solutions --> the new functionality offered by a new application or other service
					2. Tools and systems for management information -->  support and automate processes
					3. Architectures --> have existing systems, and the new or changed service will need to be compatible with them
					4. Measurement systems --> metrics
					5. Processes --> Each new or changed service will require processes
					
Service Design Processes
		1. Service Catalog Management
					- A database or structured document with information about all live IT services, including 
					those available for deployment
		2. Availability Management
		3. Capacity Management
					- responsibilities of the service provider is to ensure that the service is able to cope with the demands put upon it
		

#########################################################################################################################################################


### Service Trasition
		-  delivery of a new or changed service into the live operational environment
		-  service transition processes are used throughout the life cycle to provide control over the live 
		operational environment

Objective : 
		- Plan and manage changes to services, the introduction of new services, or the retirement of services efficiently and effectively
		- Manage risk associated with new, modified, or retired services being transitioned
		- Successfully deploy releases into the live environment
		- Provide relevant and good-quality knowledge and information about the services and service assets

Scope : 
		- The service transition stage provides guidance on the development and improvement of the 
		capabilities required to deliver new services into the live environment
		- This covers the planning, build(--> compile --> packaging),test,evaluation, implementation, and deployment of new services or 
		changes to existing services
		
## Change Management

		- ITIL states that the purpose of this process is “to control the lifecycle of all changes, enabling beneficial
		changes to be made with minimum disruption to IT services.

Objective :
		- This objective specifically states that while responding to the changing business needs, you should be 
		reducing incidents, disruption to the services, and rework
		- It is an important objective to support your business, and as organizational needs evolve, the IT department 
		or service provider should continue to ensure that the services align to business needs
	
Scope :
		- The ITIL framework defines a change as “the addition, modification or removal of anything that could have an 
		effect on IT services.”
		- This means the scope of this process covers everything from the architecture and infrastructure to the processes,
		documentation, metrics, and tools that support your services, as well as changes to IT services and configuration items
	
Type of Change
		- Many different types of change exist in operational environments
		- A change is captured as a formal request for alteration of a configuration item
		- A request for change is commonly referred to as an RFC
		- Types:
				1. Standards Change
				2. Emergency Change
				3. Normal Change
				
## Knowledge Management
		- Knowledge management is a process that impacts the wider service lifecycle, not just service transition
		- In this process, you will be considering the purpose, objectives, and scope of knowledge management,
		as well as the concepts of the Data-Information-Knowledge-Wisdom (DIKW) model and the SKMS
		
		
Objective :
		- Improve the quality of decision making throughout the service lifecycle
		- Maintain a service knowledge management system (SMKS)

## Deployment Management :

		- The role of release and deployment management in this lifecycle stage is to ensure the successful introduction 
		of changes into the live environment, minimizing the unpredicted impact to the business
		- To deliver releases into the live environment successfully, you need to have control over the release management 
		activities, and in this process we cover the purpose, objectives, and scope of the process, as well as the concepts
		of the release policy and the four phases of release and deployment
		- One of the key purposes of the process is to ensure that this activity is planned, scheduled, and controlled 
		in accordance with the needs of the organization
		


#########################################################################################################################################

### Service Operation
		
		- The service operation stage is when the service is actually being delivered, and often is a much longer stage
		than the previous stages of strategy, design and transition
		- The purpose of the service operation stage of the service lifecycle is to deliver the service at the level that 
		was agreed through the service level management process
		- Service operation must deliver the service effectively but also has to ensure that the cost of that delivery 
		is within the operational costs that formed part of the original business case

Objective :
		- It is the job of the service operation phase to deliver the service as agreed on in the SLA; this ensures that 
		the business receives the level of service it expects
		- Some service outages are inevitable; service operation will work to reduce both the number and impact of outages
		- The service operation process of problem management aims to reduce repeat incidents that disrupt business activities, 
		while incident management aims to resolve those incidents that do occur as quickly as possible
		
Scope :
		- The scope of service operation described in the ITIL framework includes the “processes, functions, organization,
		and tools” that are used to deliver and support the agreed services
		- This lifecycle stage is responsible for performing the critical day-to-day activities and processes that ensure
		the service meets the business requirement and enables the business to achieve its objectives
		
		- The ITIL Service Operation publication provides guidance on the successful management of the following:
				1. The Services Themselves --> All the activities required to deliver the services consistently within the agreed service levels
				2. The Services Management Processes --> e operation processes of event, incident, problem and access management, and request fulfillment
				3. The Technology Delivering --> Service operation is responsible for managing the technology that delivers the services
				4. The People -->  Despite automation, service operation depends on the actions of the support staff members to ensure the service runs as it should
				
Value it adds to the business :	

		- Financial savings from reduced downtime as a result of the implementation of the service operation processes 
		of problem and incident management
		-  Service operation includes the production of management information regarding the efficiency and effectiveness
		of the service delivery
		
## Processes :
		
		1. Technical Management (operation) :
				- Whatever the name given to the team or teams in any particular organization (infrastructure support, technical support,
				network management, and so on), the function referred to in the ITIL framework as technical management is required to
				manage and develop the IT infrastructure
				-This function covers the groups or teams that together have the technical expertise and knowledge to ensure that the 
				infrastructure works effectively in support of the services required by the business
				- It is responsible for managing the IT infrastructure. This would include ensuring that the staff members performing 
				this function have the necessary technical knowledge to design, test, manage, and improve IT services
		
		2. Application Management :
				- This function shares many features with the technical management function, although in this case it is the application 
				software that is supported and managed throughout its lifecycle, rather than the infrastructure
				- Application management and applications development are not the same, and it is important to understand the differences 
				between them:
								1. Application management is involved in every stage of the service lifecycle, from ascertaining the 
								requirements through design and transition and then operation and improvement
								2. Application development is mostly involved in single, finite activities, such as designing and
								building a new service
		
		3. Service Desk Organization :
				- best structure for the service desk is dependent upon the size and structure of the organization
				- Global organization will have different needs from one with all its employees based in same location
				
				1. Local Service Desk --> Service desk co-located with the users it serves; an organization with three offices would have three local service desks
				2. Centralized Service Desk --> more common structure for service desks is that of a centralized service desk
				3. Virtual Service Desk --> Calls and emails are distributed across the staff members as if they were in one centralized location
		
		4. Incident Management :
				- In ITIL terminology, an incident is defined as an unplanned interruption to an IT service, a reduction in the
				quality of an IT service, or a failure of a CI that has not yet impacted an IT service
				- This is an important definition; because the incident is an interruption to service, restoring the service or 
				improving the quality of the service to agreed levels resolves the incident
				- Note that incident resolution does not necessarily include understanding why the fault occurred or preventing 
				its recurrence; these are matters for problem managemen
				
				- Managing incidents : 
						1. Incident Identification
						2. Incident Categorization
						3. Incident Prioritization
						4. Initial Diagnosis
						5. Incident Escalation
						6. Investigation and Diagnosis
						7. Resolution and Recovery
						8. Incident Closure
		
		5. Problem Management :
				- a problem is defined as an underlying cause of one or more incidents
				- Problem management is the process that investigates the cause of incidents and, wherever possible, 
				implements a permanent solution to prevent recurrence
				
				-Problem Management Process :
						1. Detecting Problems
						2. Logging Problems
						3. Categorizing Problems
						4. Prioritizing Problems
						5. Investigating and Diagnosing Problems
						6. Identifying a Workaround
						7. Raising a Known Error record
		
		6. Event Management :
				- Modern infrastructure management depends to a large extent on the use of event monitoring tools
				- These tools are able to monitor large numbers of configuration items simultaneously, identifying any issues
				as soon as they arise and notifying technical management staff
				- The process of event management is responsible for managing events throughout their lifecycle
				- An event can be defined as any change of state that has significance for the management of a configuration item (CI) 
				or IT service
				
				
				- Event Monitoring Tools :
						
						1. Active Monitoring tools :
								- Will poll devices to check that they are working correctly
								- The tool will send a message and expect a positive response within a defined time, such as sending a “ping” 
								to a device
								- A failure to respond will be notified to support staff
						
						2. Passice Monitoring tools :
								- Do not send out polling messages
								- They detect events generated by CIs and correlate them
		
		7. Access Management :
				- Access management is the process of granting authorized users the right to use a service while preventing access to 
				nonauthorized users
				- It also is sometimes referred to as rights management or identity management
				- Access requirements can change frequently, and service operation is responsible for granting access quickly, 
				in line with the needs of the business, while ensuring that all requests are properly authorized
				- Manage access to services, carrying out the policies defined within information security management 
				- Ensure that all requests for access are verified and authorized. This may include requests to restrict or remove access
				
######################################################################################################################################################################

### Continual Service Improvement (CSI)
1. Training and awareness  --> Ensure that all staff are knowledgeable about CSI practices and understand their importance.
2. Ongoing scheduling --> Establish a regular schedule for reviewing and improving services.
3. Roles created --> Define specific roles and responsibilities for CSI to ensure accountability and effective execution
4. Ownership assigned --> Assign clear ownership for CSI activities to ensure that improvements are implemented and sustained.
5. Activities identified to be successful --> Identify and prioritize activities that are critical for the success of CSI.

####################################################################################################################################################################


# What is Data Center?
	A data center is a building that contains a large amount of computer hardware. This hardware consists of the following:
			1.CPU	--> brains of the data center, and is made up of literally thousands of processors 
			2.Storage --> his is static storage the system has at its disposal. This is typically a combination of hard disk drives (regular storage),
			solid state drives (high-speed storage), and tape drives (backup).
			3.Communication --> this can consist of modems (telephone line communications), datasets (dedicated telephone line communications),
			traditional networking , and high-speed networking (fiber optics or similar)
			4.Software --> falls into two categories
								1.infrastructure software -->  manage the hardware, database management systems, email systems
								2.application software --> programs that employees typically use on a day-to-day basis
								
## why data center is important to business?
			1.Email and file sharing
			2.Productivity applications
			3.Customer relationship management (CRM)
			4.Enterprise resource planning (ERP) and databases
			5.Big data, artificial intelligence, and machine learning
			6.Virtual desktops, communications and collaboration services
		
# Data Center Architecture Overview

			- data center infrastructure design is critical, and performance, resiliency(quickly recoverable), and 
			scalability need to be carefully considered
			-  data center network design is based on a proven layered approach, which has been tested and improved
			over the past several years in some of the largest data center
			- The layered approach is the basic foundation of the data center design that seeks to improve scalability, 
			performance, flexibility, resiliency, and maintenance
					
						1.Core Layer :
									- Provides the high-speed packet switching backplane for all flows going in and out 
									of the data center
									- core layer provides connectivity to multiple aggregation modules and provides a resilient 
									Layer 3 routed fabric with no single point of failure.
									- The core layer runs an interior routing protocol, such as OSPF or EIGRP, and load balances 
									traffic between the campus core and aggregation layers using Cisco Express Forwarding-based hashing algorithms
						
						2.Aggregation layer :
									- Provide important functions, such as service module integration, Layer 2 domain definitions,
									spanning tree processing, and default gateway redundancy
									- Server-to-server multi-tier traffic flows through the aggregation layer and can use services,
									such as firewall and server load balancing, to optimize and secure applications
									- The smaller icons within the aggregation layer switch in represent the integrated service modules. 
									These modules provide services, such as content switching, firewall, SSL offload, intrusion detection,
									network analysis, and more.
									
						3.Access layer :
									- Where the servers physically attach to the network
									- The server components consist of 1RU servers, blade servers with integral switches, blade servers 
									with pass-through cabling, clustered servers, and mainframes with OSA adapters
									- The access layer network infrastructure consists of modular switches, fixed configuration 1 or 2RU 
									switches, and integral blade server switches.
									- Switches provide both Layer 2 and Layer 3 topologies, fulfilling the various server broadcast 
									domain or administrative requirements.
									
# Data Centre Requirements :
			
			1. Servers --> 	three major types of servers are blade servers, rack servers and tower servers
			2. Racks --> Server racks are of two major kinds – Open Racks and Cabinets – and support the physical weight of the servers,
			storage devices and switches
			3. Cables --> Coaxial, twister and fiber optic cables are used in data centers to connect switches,
			storage devices and servers.Fiber optic cables
			4. Switches --> switches are responsible for linking multiple devices together
			5. Storage --> NAS (Network Attached Storage) and SAN (Storage Area Network) are the two most commonly used types of storage systems
			6. Power Equipment --> Data centers require 24x7 power availability and must therefore employ backup generators 
			and multiple data routes to ensure Uninterrupted Power Supply (UPS)
			7. Security Systems --> physical space where the data center is set up must be resistant to natural disasters like flooding, earthquakes, fires, etc
			8. Cooling Systems --> s. Server rooms must therefore be maintained at a specific temperature as prescribed and 
			planned during setup, through the use of external HVAC equipment, for smooth functioning
			9. Policies and Procedures --> Maintenance and management of the physical hardware requires the strict adherence to certain policies and procedures 
			that need to be followed. This could include scheduled servicing, downtime protocols, or even housekeeping
		

# Data Center Prerequisite :
			
			1. Structure
			2. Cabling performance
			3. Redundancy 
			4. Grounding/potential equalization
			5. Tier classification
			6. Cable routing
			7. Ceiling and double floors
			8. Floor load
			9. Space requirements (ceiling height,door width)
			10.Power supply/UPS
			11.Fire protection/safety
			12.Cooling
			13.Lighting
			14.Administration/labeling
			15.Temperature/humidity
			

## Required Physical Area for Equipment and Unoccupied Space
			
			1. Leave Room for Growth
					- Floorspace
					- Power Requirements
					- Cooling Requirements
					- Server Space 
			2. Plan for the Support Team
			3. Optimize Data Center Cooling
					- Traditional Air Conditioning
					- Water Cooling Units
					- Outdoor Air Cooling
					- Localized Cooling
			4. Smart Data Center Airflow Managment
					- Main Intake And Exhaust
					- Server Rack Airflow
					- Segemented Aisles
			5. Don't Neglect Physical Security 
			6. Focus on Proper Wiring from the Beginning
			7. Plan Properly for Your Data Center Design
			8. Focus on proper wiring from the beginning
			9. Plan Properly for Your Data Center Design
			
## Selecting a Geographic Location
			1. Disaster Avoidance
			2. Network Carrier Availability
			3. Availability of Power
			4. Transport / Accessibility 
			5. Land and Building cost
			6. Tax Structure, Incentives and Subsidies
			7. Availability of Skilled Man power
			8. Safely and Security 
			9. Urban Planning and Environment 
			10.Climate Conditions
			
	
## Selecting and Existing building

	- first best practices are in the selection process
				1. Define a location selection process to list critical and desirable selection criteria
				2. Remember that the selection process should not be limited to the situation today
		
	
	- Security and Safety Considerations
				- Security and safety are typically the highest priority in data center site selection
						1. Avoid high risk areas
						2. Choose a site that has easy access for emergency responders
						3. Choose a site with good air quality 
						4. In a building with multiple tenants, prefer the end of the building
	
	- Building Considarations :
				1. For existing construction, single story building with large floor areas are often best
				2. Check there is sufficient area around the building for parking, water, and fuel storage, as well as for access for delivery trucks
	
	- Real Estate and Negotiations Concerns
				1. Besides aiming for a cost per square foot that falls within your budget,consider possible add-on costs for upgrading power, 
				networking, and any other necessary facilities
				2. For speedier negotiations, prefer sites with single owners, rather than multiple
				3. Check on the availability of public incentives, offered for example by municipalities to attract high-technology businesses.
				4. Make sure you know about any site or zoning restrictions that could affect the type of building or operations you plan
				(for example, the operation of diesel generators.)
	
	- Climate Conditions
				1.  Get weather data for a sufficiently long period (10 years) to understand if there is 
				a history of natural disasters. Avoid such sites
				2. Consider sites that make outside air cooling viable, thus lowering a major cost in data center operation.
				3. Check that the site humidity ranges are compatible with the IT equipment you plan to use in the data center 
				(or face extra costs to make this so inside the data center).
				
	- Network Connectivity and Power Considerations
				1. Ensure you have adequate networking and power for your needs today, and that these facilities can keep pace 
				with your needs into the future
				2. Prefer sites that offer redundant, separate network links and power lines. The best is to have the entry points
				on opposite sides of the building (north and south for power, east and west for networking, for example.)
				
	- Staffing Considerations
				1. Moderate economic conditions are often a good trade-off, with sufficient access to skilled people, but without 
				other expenses driving up overall costs.
				2. Locating a data center near a university or IT training school can ensure the availability of staff with 
				appropriate skills.
				3. A site offering good commute times and quality of life, in general, will help avoid staff turnover.
				


##################################################################################################################################################################################
### Infrastructure in data center####################################
#####################################################################

# Modular Cabling Design
	
	Modular cabling design is a structured approach to data center cabling that emphasizes modularity, scalability, and flexibility. The key aspects of a modular cabling design include:
		
		1. Structured Cabling
				- A structured cabling system uses a Main Distribution Area (MDA) as the central point where all connections are routed. 
				- This creates a organized, standardized cabling infrastructure that is easier to manage and scale compared to a point-to-point "spaghetti"
				cabling approach.
		2. Modularity and Scalability
				- Modular cabling systems use components like trunks, harnesses, and enclosures that can be easily added, removed, or rearranged as the 
				data center's needs change. This allows the cabling infrastructure to scale up or down without major disruptions.
	    3. Improved Airflow and Efficiency
				- Structured, modular cabling takes up less physical space and allows for better cable management. This improves airflow through the 
				data center racks, which enhances cooling efficiency and extends the life of equipment.
		4. Faster Moves, Adds, and Changes
				- The organized nature of modular cabling makes it quicker and easier to make changes to the infrastructure, reducing downtime 
				during maintenance or expansions.

# Data Center Cabling Designs/Topologies
		
		1. Centralized Cross-Connect Topology: 
				- Centralized cross-connect data center cabling can be adopted in two ways:
						
						1. Centralized Cross-Connect via MDA :
									
								-   the backbone cable directly connects the switches from the main distribution area (MDA) to the 
									horizontal distribution area (HDA), which is further centralized by cross-connecting the distribution 
									rows or lines from HDA to the equipment distribution 
								-   The distribution lines from HDA to the EDAs are connected by using three-connector channels, which allows 
								    the HDA lines to terminate at patch panels and replicate or mirror it to the switch ports of EDA areas (EDAs)
								-   The termination point of the first patch panel is further cross-connected with the second patch panel so that
    								each switch port from individual EDAs are cross-connected in the network.
					    
						2. Centralized Cross-Connect via ZDA :
						
								-   implement this type of centralized cross-connect data center cabling, a four-connector channel is utilized. 
									At first, the backbone cable connects the zone distribution area (ZDA) to the HDA
								- 	Further, by using a four-connector channel, the ZDA distribution lines are cross-connected to the patch panels
									of EDAs. However, the patch panels of the four-connector channels are interconnected with a cross-link and one 
									or/and both links are cross-connected to the switch ports of different EDAs
								-	Although centralized cross-connect via ZDA type of data center cabling is not widely adopted, it still can be 
									used for data centers with critical security needs. It secures the cabling by efficient cooling and increases 
									efficiency due to the easy reconfiguration feature
					    
						3. Benefits :
								-  	This infrastructure enables easy connections from any switch to any device in the data center cabling network.
								-	Permanent connections to the switches can be established in order to avoid over-interaction with sensitive devices.
								-	The cabling infrastructure enables easy configuration and reconfiguration, device addition, line integration, and so on.
								- 	Network flexibility and manageability are higher in these topologies.
								-	The centralized cross-connect topology assures optimized space utilization which reserves server cabinet spaces.
								
		2. Partially Centralized Inter-Connect Topology: 
				- data center cabling features a backbone cabling from the main distribution area that runs through multiple HDAs in a row of specific EDA. 
				  While running through HDAs the backbone cable inter-connects the switches from each HDA with the help of two-connector channels
				- The second distribution lines from the two-connector channels are inter-connected to the patch panel of the EDA. This topology is 
				can be implemented in either of the following ways :
						
						1. Point-to-Point: To implement the point-to-point approach to partially centralizing inter-connect topology, the switches from HDAs 
						are allocated at a distance from the EDA. However, no cross-connect or inter-connect approach is deployed in this type.
						Long patch chords are used to connect the switches from HDAs with the patch panels in the EDA.
						
						2.One-Connector Channel: To implement a one-connector channel approach, one-connector channels are used instead of two-connector channels.
						The patch panels are only deployed in the HDAs and then they are directly connected to different equipment housed in the EDA. This approach 
						is suitable for smaller data center applications. It also allows complete utilization of mounting rail in the server cabinets.
					
				- Benefits :
						- It reduces the requirement of copper or optical cabling run.
						- It optimizes the pathway spaces and increases the safety of the connections.
						- The number of terminations is comparatively lesser than other types of cabling 
						  due to a lesser number of switches and patch panels being	interconnected.
						- It allows the switches and patch panels to locate in the adjacent cabinets without 
						  jeopardizing the cabling efficiency.
		
		3. Distributed In-cabinet Switching:
				- In the distributed in-cabinet switching topology of data center cabling, the connections are simplified by 
				  eliminating the use of HDAs totally
				- This topology features a backbone cable that connects the MDA directly to different cabinets or switches of EDAs.
				  This way, the backbone is directly connected to performing devices in the data center connections. 
				- This type of cabling also features a direct connection to the storage area network (SAN) which further
				  simplifies the data storage operations in the data center.
				
				- Benefits :
						- This type of cabling drastically reduces cabling complexity and expenses.
						- For applications with high-bandwidth like 40gbps to 100gbps, this topology gives maximum returns on investment (ROI).
						- In special applications where a specific functional area of the data center is required to be separated or secured.
						- It is an effective solution for limited cabinet space availability.


# Important Devices for Data Center Cabling
		
		1. Media Converter -->  Copper to fiber media converters and fiber to fiber media converters are two popular types of media converters
		2. Network Switch --> 
		3. Optical Transceivers --> with SFP modules are used to extend distances, reduce power consumption, optimize cabling density, and leverage existing multimode fiber
	

# Guidelines to Improve the Performance of Data Center Cabling
		-Utilize pre-terminated cabling to perform the connections. It enables plug-and-play operation which saves efforts and time on cabling integrations.
		-Use high-quality interconnecting products like media converters, optical transceivers, networking switches, etc.
		-Adopt flexible yet structured types of cabling topologies.
		-Utilize back-to-back vertical cable managers to secure cable bulking at high-density locations.
		-Form patching zones between the equipment and cable distribution lines in order to identify and categorize the elements.
		-Prevent cable slack by using Velcro ties and guide the patch chords thoroughly



# Points of Distribution
		
		1. Main Distribution Area (MDA): 
				- The main distribution is area is where functional components are housed. It houses switches, routers, cross-connect, 
				and inter-connect equipment, etc. The LAN essentials are housed in the MDA
		2. Horizontal Distribution Area (HDA): 
				- HDA houses the connecting equipment and switches to interconnect equipment distribution area (EDA) to local area network (LAN),
				storage network area (SAN), and/or KVM switches
		3. Zone Distribution Area (ZDA): 
				- ZDA is basically a coherent point between the HAD and EDAs.
		4. Equipment Distribution Area (EDA): 
				- EDA is the functional zone of the data center that houses end equipment like servers, racks, cabinets, etc. Also, the HAD 
				is terminated in the EDA region by using patch panels
		5. Backbone Cabling:
				- Backbone cabling is the cable line that interconnects all other functional areas of the data center cabling. Error in backbone 
				cabling can terminate the function of all other functional zones of data center cabling
				
Taking all the above-mentioned factors into consideration, commonly three topologies of data center cabling are adopted across industries. 
These strategically planned topologies ensure lesser possibilities of errors, easy maintenance, and highly-efficient performance of the data center infrastructure


# ISP Network Infrastructure and WAN Links
		- The data center's connection to the outside world is provided by the ISP network infrastructure and WAN links.
		  Key considerations include:
				1. Redundancy: Ensuring there are multiple diverse WAN links for high availability.
				2. Bandwidth: Provisioning sufficient bandwidth to handle the data center's connectivity needs.
				3. Security: Implementing robust firewall, VPN, and other security measures at the network edge.
				4. Monitoring: Closely monitoring the ISP links and network performance.

# Network Operations Center and Monitoring
		- The Network Operations Center (NOC) is the central hub for monitoring and managing the data center's entire network infrastructure.
		  Key functions include:
				1. Real-time Monitoring: Tracking network performance, security events, and infrastructure health.
				2. Incident Management: Quickly identifying and resolving any network-related issues or outages.
				3. Change Management: Coordinating and executing planned changes to the network.
				4. Reporting and Analytics: Generating reports and insights to optimize network operations.


# Data centre Logical security

		-  logical security also includes meeting government regulations and compliance mandates. Governments – both national and local –
		   are rapidly passing new data privacy laws. These laws, Data Centre Dynamics explains, add even more complexity to the security landscape

# Reasons for Data Center Consolidation

		Data center consolidation refers to the process of reducing the number of physical data centers an organization operates. Common reasons for consolidation include:
		1. Cost Savings : 
					- Consolidating to fewer, larger data centers can reduce capital and operational expenses related to facilities, power, cooling, and staffing.
		2. Improved Efficiency
					- Larger, more modern data centers tend to be more energy-efficient and easier to manage than smaller, distributed facilities.
		3. Standardization
					- Consolidation allows an organization to standardize its IT infrastructure, processes, and management, leading to greater efficiency. 
		4. Increased Resiliency
					- Fewer, centralized data centers are often easier to secure and protect against outages and disasters compared to multiple distributed sites.
		5. Compliance and Governance
					- Consolidation can simplify compliance with regulations and improve overall governance of the IT environment.
					
	- Consolidation Opportunities :
		- The main areas of opportunity for data center consolidation include:
	
					1.Server Consolidation	: Migrating workloads from underutilized or outdated servers onto newer, more powerful hardware.
					2.Storage Consolidation : Combining disparate storage systems and platforms into a centralized, virtualized storage infrastructure.
					3.Network Consolidation : Streamlining network equipment, topology, and management by moving to a more centralized model.
					4.Service Consolidation : Migrating application and IT services from distributed systems to centralized, shared platforms.
					5.Process Consolidation : Standardizing and centralizing IT management processes, such as provisioning, monitoring, and change control.
					6.Staff Consolidation	: Reducing the number of distributed IT teams by consolidating to a centralized, shared services model.
					7.Data Consolidation	: Migrating data from multiple, disparate sources into a centralized data lake or data warehouse.
					
# Data Consolidation Phases
		- Data center consolidation typically involves several phases to ensure a smooth transition:
					1.Inventory and Assessment: Identify and assess all IT assets, including hardware, software, and network infrastructure.
					2.Planning and Design: Develop a comprehensive plan for consolidation, including timelines, roles, and responsibilities.
					3.Migration and Implementation: Migrate data and applications to the new, consolidated environment.
					4.Testing and Validation: Test and validate the consolidated environment to ensure it meets the organization's requirements.
					5.Post-Migration Support: Provide ongoing support and maintenance to ensure the consolidated environment remains stable and efficient.

#Data Center Servers
		- Data center servers are the core compute resources that power applications, services, and workloads. Key considerations include:
					
					1.Server Capacity Planning : Accurately forecasting future compute requirements to ensure the right server capacity is provisioned.
					2.Server Virtualization	: Leveraging hypervisor technologies to consolidate multiple virtual machines onto fewer physical servers.
					3.Server Hardware Selection : Choosing the appropriate server models, CPU, memory, and storage configurations based on workload requirements.
					4.Server Redundancy	: Implementing redundant server components and clustering/failover capabilities for high availability.

#Disaster Recovery
		- Disaster recovery (DR) planning is essential for data centers to ensure business continuity in the event of a major outage or disaster.
		  Key elements include:
					
					1.Backup and Replication :
								- Implementing robust backup and data replication strategies to protect against data loss.
					2.Failover and Failback
								- Establishing automated failover procedures to quickly shift operations to a secondary DR site, and failback when the primary site is restored.
					3.Testing and Validation
								- Regularly testing the DR plan to ensure it functions as expected and identifying any gaps or weaknesses.
					4.Alternate Site Selection
								- Choosing a suitable secondary site location that is geographically separated from the primary data center.
					5.Recovery Time Objectives (RTOs) and Recovery Point Objectives (RPOs)
								- Defining the acceptable downtime and data loss thresholds to guide the DR strategy.

#################################################################################################################################################################################
###  Devops    ###########################################################
##########################################################################

# Introduction to Virtualization

	- Virtualization is a technique to divide the computer resources logically. 
	  It’s achieved by abstracting away the underlying complexity of resource segregation
	  
	- The hypervisor manages the virtualization technique and creates, runs, and monitors multiple virtual machines (guest)
	  simultaneously, on single computer hardware (host)
	 
	- So, hypervisors regulate the virtualization process, creates multiple virtual machines that allow you to work on several
	  computing instances at once. This is the key difference between Virtualization and Hypervisors
	  
	- Virtual Machine Monitor or VMM or a Hypervisor act as a supervisor. It’s implemented on computer hardware as code embedded
      in a system’s firmware or as a software layer
	  
	- Hypervisors create, start, stop, and reset multiple VMs while virtually sharing its resources like RAM and Network interface controller.
	
	- VMM governs the guest operating systems and manages execution on a virtual operating platform. It furthermore separates Virtual Machines (VMs)
      from each other logically, so even if one OS crashes for some reason, the other VMs can function unhindered.

# Virtualization Types: 	Type 1 and Type 2

		1. Type 1 : Bare Metal hypervisor
		2. Type 2 : Hosted hypervisor
	

# Type 1 : Bare Metal hypervisor

		- A.K.A : Bare Metal or Native
		- Defination : Run directly on the system with VMs running on them
		- Virtualization : Hardware Virtualization
		- Operation : Guest OS and application run on the hypervisor
		- Scalability : Better Scalability
		- Setup/Installation : Simple, as long as you have the necessary hardware support
		- System Independence : Has direct access to hardware along with virtual machines it hosts 
		- Speed : Faster
		- Performance : Higher-performance as there's no middle layer
		- Security : More Secure
		- Example : VMware ESXi,Microsoft Hyper-V,Citrix XenServer
		
# Type 2 : Hosted hypervisor

		- A.K.A : Hosted hypervisor 
		- Defination : Runs on the conventional Operating System
		- Virtualization : OS Virtualization
		- Operation : Runs as an application on the host OS
		- Scalability : Not so much, because of it reliance on the uderlying OS.
		- Setup/Installation : Lot simpler setup, as you already have an Operating System 
		- System Independence : Are not allowed to directly access the host hardware and it resources
		- Speed : Slower because of the system's dependency
		- Performance : Comparatively has reduced performance rate as it runs with extra overhead
		- Security : Less Secure, as any problem in the base operating system affect the entire system including the protected hypervisor
		- Example : VMware Workstation Player, Microsoft Virtual PC, Sun's VirtualBox
	

## Virtualization, Hardware Virtualization, Para-Virtualization, Cloning, Snapshot,and Template

		1.Virtualization : Virtualization is a technology that allows the creation of multiple virtual instances of a resource, 
		  such as an operating system, server, storage device, or network, on a single physical system. It provides an abstraction 
		  layer that decouples the physical hardware from the software running on it, enabling more efficient utilization of system 
		  resources and increased flexibility.
		
		2.Hardware virtualization : also known as server virtualization, is a specific type of virtualization that involves the 
		  creation of virtual machines (VMs) on a physical server. A hypervisor, which is a software layer, manages the virtual 
		  machines and provides them with access to the underlying hardware resources, such as CPU, memory, and storage.
			
			- There are three main types of hardware virtualization:
					1. Full Virtualization : In full virtualization, the hypervisor completely simulates the underlying hardware, 
					   allowing unmodified guest operating systems to run in virtual machines. The guest OS is unaware that it is
					   running in a virtualized environment.
					
					2. Paravirtualization : the hypervisor does not simulate the entire hardware, but instead 
					   provides a specialized API (hypercalls) for the guest OS to communicate with the hypervisor. The guest OS is 
					   aware that it is running in a virtualized environment and is modified to use these hypercalls.
					   
					3. Hardware-assisted Virtualization : also known as native virtualization, utilizes specialized CPU instructions
                       provided by the hardware (e.g., Intel VT-x, AMD-V) to aid the virtualization process. This makes the hypervisor 
					   implementation less complex and more efficient.

		3. Cloning : Cloning in the context of virtualization refers to the process of creating an identical copy of a virtual machine.
           This can be useful for tasks such as:
					- Rapid deployment of new VMs with the same configuration
					- Backup and disaster recovery
					- Testing and development environments
		   Cloning can be performed at the image level (creating a full copy of the VM's disk image) or at the configuration level 
		   (copying the VM's settings and metadata).
		
		4. Snapshot : Snapshots in virtualization are a way to capture the state of a virtual machine at a specific point in time. 
		   This includes the VM's memory, disk state, and configuration. Snapshots can be used for:
			    1. Reverting to a previous state of the VM (e.g., before a system change or update)
				2. Creating backup points for the VM
				3. Testing and experimenting with changes without affecting the production environment
				
		5. Template : A virtual machine template is a pre-configured virtual machine image that can be used to quickly deploy new VMs
           with a standardized configuration. Templates typically include the operating system, installed applications, and any necessary
		   configurations.
		
Operating System Virtualization
		
		- Operating System Virtualization, also known as OS-level virtualization, is a type of virtualization technology that allows a single
          operating system to run multiple isolated user-space instances, called containers or virtual environments. This is in contrast to 
		  traditional hardware-based virtualization, where a hypervisor creates and manages multiple virtual machines (VMs), each with its own
		  operating system.
		  
Cluster Architecture : Cluster architecture refers to a system of interconnected computers or servers that work together as a unified computing resource. 
			The key aspects of cluster architecture are:
			Cluster Components : 
					1. Cluster Nodes: The individual computers or servers that make up the cluster. Each node has its own memory, CPU, and operating system.
					2. Cluster Operating System: The software that manages and coordinates the nodes in the cluster, making them function as a single system.
					3. Network Interconnect: The high-speed network that connects the nodes, enabling efficient communication and data transfer between them.
					4. Load Balancer: Software or hardware that distributes incoming requests across the nodes in the cluster to prevent any single node from
                       being overloaded.
			Types of Cluster Architectures
					1. High-Availability (HA) Clusters: These clusters are designed for fault tolerance, ensuring uninterrupted service
  					   even when one or more nodes fail. They use redundant hardware and software to automatically failover to backup nodes.
					2. Load-Balancing Clusters: These clusters distribute incoming workloads across multiple nodes to maximize performance 
					   and prevent any single node from being overburdened. The load balancer ensures even distribution of requests.
					3. High-Performance Clusters: Also known as "Beowulf clusters", these are designed for parallel processing of computationally
  					   intensive tasks. They leverage the combined processing power of multiple nodes to tackle complex problems faster.
			Benefits of Cluster Architecture
					1. High Performance: Clusters can provide significantly higher processing power and throughput compared to a single 
					   server by aggregating the resources of multiple nodes.
					2. Scalability: Clusters can be easily scaled by adding more nodes to the system, allowing the computing capacity to grow as needed.
					3. Fault Tolerance: If one node fails, the cluster can continue operating by redistributing the workload to the remaining nodes,
                       ensuring high availability.
					4. Load Balancing: Clusters use load balancing techniques to distribute incoming requests across multiple nodes, preventing any 
					   single node from becoming a bottleneck.
					5. Cost-Effectiveness: Clusters can be built using commodity hardware, making them more cost-effective than traditional high-end 
					   servers or mainframes.
					6. Flexibility: Clusters can be configured to handle a wide range of applications, from web hosting to scientific computing, 
					   based on the specific requirements.
					   
####################################################################################################################################################################
### Storage Area Network  #######################################################
#################################################################################

# Configuring a SAN with FreeNAS
			 - FreeNAS is a popular open-source software platform for building network-attached storage (NAS) and 
			   storage area network (SAN) systems. It provides a user-friendly web interface for configuring and managing storage resources.
			 
			Practical Steps :
						1. Install FreeNAS on a dedicated server or virtual machine. This involves booting from a FreeNAS installation image
						   and following the guided setup process.
						2. Configure network settings, including IP addresses and DNS, to connect the FreeNAS system to your network.
						3. Create storage pools and datasets using ZFS, FreeNAS's advanced file system. This allows you to configure
     					   RAID configurations for redundancy and performance.
						4. Set up iSCSI targets to expose block storage to your VMware ESXi hosts over the network. This allows the
   						   ESXi hosts to mount the FreeNAS storage as virtual disks.
						5. Configure multipathing on the ESXi hosts to provide high availability and load balancing for the iSCSI storage.
            
			ZFS Volume Configuration :
						- FreeNAS uses the ZFS file system, which provides advanced features like data integrity, snapshots, and thin provisioning.
						  You can create storage pools using RAID configurations like RAIDZ1, RAIDZ2, or RAID10 to balance capacity, performance, and redundancy.
			
			IP-Based Storage Communication :
						- FreeNAS supports various IP-based storage protocols, including iSCSI for block-level access and NFS/SMB for file-level access.
						  This allows you to connect storage resources to a variety of clients, including VMware ESXi hosts, over the Ethernet network.

			Object Storage Services : 
						- In addition to block and file storage, FreeNAS can also provide object storage services using protocols like S3 and Swift.
						  This allows you to store and access unstructured data in a scalable and highly available manner.
						  
####################################################################################################################################################################
### Cloud Computing   #######################################################
#################################################################################

# Introduction to Cloud Computing
			
			- Cloud computing is a technology that allows users to access and use computing resources, such as storage,
			  processing power, and software applications, over the internet instead of on local devices or on-premises 
			  infrastructure. This model enables on-demand access to a shared pool of configurable computing resources 
			  that can be rapidly provisioned and released with minimal management effort.

# Cloud SPI (software, platform, infrastructure) Model

			1. Infrastructure as a Service (IaaS): In IaaS, we can rent IT infrastructures like servers and virtual machines (VMs)
			   , storage, networks, operating systems from a cloud service vendor. We can create VM running Windows or Linux and install
			   anything we want on it. Using IaaS, we don’t need to care about the hardware or virtualization software, but other than that,
 			   we do have to manage everything else. Using IaaS, we get maximum flexibility, but still, we need to put more effort into maintenance.
			
			2. Platform as a Service (PaaS): This service provides an on-demand environment for developing, testing, delivering, and managing software 
			   applications. The developer is responsible for the application, and the PaaS vendor provides the ability to deploy and run it. 
			   Using PaaS, the flexibility gets reduce, but the management of the environment is taken care of by the cloud vendors.

			3. Software as a Service (SaaS): It provides a centrally hosted and managed software services to the end-users. It delivers software 
			   over the internet, on-demand, and typically on a subscription basis. E.g., Microsoft One Drive, Dropbox, WordPress, Office 365, 
			   and Amazon Kindle. SaaS is used to minimize the operational cost to the maximum extent.

# Cloud Computing Deployment Models
			
			1. Public Cloud : The public cloud is owned and operated by a third-party cloud service provider, such as Amazon Web Services (AWS),
     		   Microsoft Azure, or Google Cloud Platform. The resources are shared among multiple tenants, and users access these resources over the internet.

			2. Private Cloud : The private cloud is owned and operated by a single organization, either on-premises or hosted by a third-party provider. 
			   The resources are dedicated to that organization and are not shared with others.

			3. Hybrid Cloud : The hybrid cloud is a combination of public and private cloud environments, where organizations can use both on-premises and
               off-premises resources to meet their computing needs. This allows them to take advantage of the scalability and cost-effectiveness of the 
			   public cloud while maintaining control over sensitive data and applications in the private cloud.

# Cloud Security
			
			1. Service Level Agreements (SLAs) : SLAs define the level of service and performance guarantees provided by the cloud service provider, 
			   including availability, reliability, and security measures.
			   
			2. Identity and Access Management (IAM) :IAM controls who has access to cloud resources and what actions they can perform, 
			   ensuring that only authorized users and applications can access sensitive data and perform critical operations.
			   
# Service Models
			- Cloud computing offers three primary service models:
						1. Infrastructure as a Service (IaaS): 
									- Example: Amazon Web Services (AWS) EC2.
									- Features: Provides virtualized servers or instances that clients can request and manage.
									  Clients are responsible for managing the operating system, middleware, and applications.
						
						2. Platform as a Service (PaaS):
									- Example: Google App Engine.
									- Features: Offers a platform for developing and deploying applications without managing 
									  the underlying infrastructure. Clients can focus on developing applications without worrying about the infrastructure.
						
						3. Software as a Service (SaaS):
									- Example: Microsoft Office 365.
									- Features: Provides applications that are accessed over the internet. Clients do not manage the infrastructure or the 
									  application itself; they only use the application.
						  
# What are the services provided by Cloud (AWS)?
			
			- Amazon Web Services (AWS) is a cloud service from Amazon, which provides services in the form of building blocks, 
			  these building blocks can be used to create and deploy any type of application in the cloud.
			  
			- few domains which are widely used are:
					1. Computer --> services related to compute workloads
							- EC2 (Elastic Compute Cloud)
							- Lambda
							- Elastic Beanstalk
							- Amazon LightSail
					2. Storage -->  services related data storage
							- S3 (Simple Storage Service)
							- Elastic Block Store
							- Amazon Glacier
							- AWS Snowball
					3. Database --> database related workloads
							- Amazon Aurora
							- Amazon RDS
							- Amazon DynamoDB
							- Amazon RedShift
					4. Migration --> transferring data to or from the AWS Infrastructure
							- AWS Database Migration Service
							- AWS SnowBall
					5. Network and Content Delivery --> used for isolating your network infrastructure, and content delivery is used for faster delivery of content
							- Amazon Route 53
							- AWS CloudFront
					6. Management Tools --> services which are used to manage other services in AWS
							- AWS CloudWatch
							- AWS CloudFomation
							- AWS CloudTrail
					7. Security & Identity Compliance --> services which are used to manage to authenticate and provide security to your AWS resources
							- AWS IAM
							- AWS KMS
							- AWS Shield
					8. Messaging --> services which are used for queuing, notifying or emailing messages
							- Amazon SQS
							- Amazon SNS
							- Amazon SES
							- Amazon Pinpoint
							
# Introduction to OpenStack
			- OpenStack is an open-source cloud computing platform for building and managing public and private clouds.
              It provides Infrastructure-as-a-Service (IaaS) by pooling together compute, storage, and networking resources in
 			  data centers and providing an interface for managing and provisioning those resources.

			Key components of OpenStack include:
					Nova - Compute service for managing virtual machines
					Neutron - Networking service for managing virtual networks
					Cinder - Block storage service for attaching volumes to VMs
					Swift - Object storage service for storing unstructured data
					Keystone - Identity service for authentication and authorization
			
			OpenStack enables organizations to build their own AWS-like cloud platforms on commodity hardware. 
			It provides APIs for developers to programmatically provision and manage resources.
			
# Hyper-Converged Infrastructure (HCI) vs Cloud

			1.Hardware : HCI combines compute, storage and networking into a single appliance	
			2.Scalability :	HCI scales by adding more nodes to the cluster	
			3.Flexibility :	HCI is more rigid with pre-configured hardware	
			4.Ownership	: HCI hardware is owned by the organization	
			5.Maintenance :	HCI requires the organization to maintain the hardware	
			6.Cost : 	HCI has higher upfront costs but lower ongoing costs	
			7.Use Cases : HCI is good for predictable workloads with known resource needs	

Cloud
			1.Hardware : Cloud uses distributed physical hardware in data centers	
			2.Scalability : Cloud scales by provisioning more VMs/containers on demand
			3.Flexibility :	Cloud is more flexible with on-demand provisioning of resources	
			4.Ownership	: Cloud resources are owned and managed by the cloud provider	
			5.Maintenance :	Cloud provider maintains the underlying infrastructure	
			6.Cost : 	Cloud has lower upfront costs but higher ongoing costs	
			7.Use Cases : Cloud is better for unpredictable, variable workloads	
			
			
####################################################################################################
## Cloud API #######################################################################################
####################################################################################################

## Cloud API integration
			- Cloud API integration is the process of connecting cloud-based applications and services using APIs to enable data exchange and automation
			
# DC/DR Migration
			- Cloud API integration is crucial for migrating data centers (DC) and disaster recovery (DR) environments to the cloud.
 			  APIs allow automated transfer of virtual machines, storage volumes, network configurations and more from on-premises to
			  cloud infrastructure like AWS, Azure or Google Cloud. This streamlines the migration process and ensures consistency between environments.

# DC/DR Storage Synchronization
			- To keep on-premises and cloud data centers in sync, APIs are used to continuously replicate storage volumes, databases and object storage 
			  between sites. This enables fast failover to the cloud DR site if the primary DC goes down. APIs automate the synchronization process 
			  without manual intervention.

# Bootstrapping Configuration Management
			- APIs are leveraged to automatically provision new cloud servers and bootstrap them with configuration management tools like Chef or Puppet.
			  This involves using APIs to create servers, attach storage, assign network interfaces and then use APIs to install and configure the CM agent. 
			  The entire process can be automated via API calls.
			  
# Physical to Cloud Migration
			- Migrating physical servers to the cloud also relies on APIs for discovery, assessment and migration. APIs are used to inventory physical servers,
			  analyze their configurations, and then automate the process of converting them to cloud-ready virtual machines that can be deployed in
			  the target cloud environment. This accelerates large-scale physical to cloud migrations.
			 
#######################################################################################################
# Centralized Logging
			- Centralized logging involves collecting and storing logs from various systems, applications, and devices in a single location.
			  This allows for easier monitoring, analysis, and troubleshooting of system activities. In a real-world scenario, a company might 
			  use a centralized logging system to monitor and analyze logs from their web servers, databases, and network devices. This helps in 
			  detecting security breaches, identifying performance issues, and optimizing system configurations.
			  
# Nagios
			- Nagios is a classic IT monitoring tool that focuses on network and infrastructure monitoring. It uses agents installed on network 
			  elements and components to collect data through a pull methodology. Nagios is known for its extensive library of plugins, making 
			  it highly customizable and suitable for traditional IT setups. For example, a company might use Nagios to monitor the performance 
			  and availability of their network switches, routers, and servers.

# Prometheus Next-Gen NMS
			
			- Prometheus is a modern, open-source monitoring tool designed for collecting and processing time-series data. It uses a pull model 
			  where applications push metrics to their API endpoints or exporters. Prometheus is highly flexible and can be integrated with various
			  tools like Grafana for visualization. It is particularly useful for monitoring cloud-native applications and microservices. 
			  For instance, a company might use Prometheus to monitor the performance and availability of their Kubernetes clusters and containerized applications.

# Identifying Bottlenecks
			- Identifying bottlenecks involves finding the slowest or most resource-intensive parts of a system to optimize performance. 
			  This can be done by analyzing metrics such as CPU usage, memory consumption, and network traffic. In a real-world scenario,
			  a company might use tools like Prometheus to identify bottlenecks in their web application by analyzing metrics like request
			  latency and error rates. This helps in optimizing the application's performance and improving user experience.

# Auto-Scaling and Auto-Rebuilding Cloud Instances
			- Auto-scaling and auto-rebuilding cloud instances are techniques used to dynamically adjust the resources allocated to a system based
			  on its current load. This ensures that the system remains responsive and efficient. For example, a company might use auto-scaling to 
			  increase the number of servers in their cloud infrastructure during peak hours and then reduce them during off-peak hours. Similarly,
			  auto-rebuilding ensures that the cloud instances are always up-to-date and running the latest versions of software.
			  
# Updating Servers Without Downtime
			- Updating servers without downtime involves applying updates and patches to servers without disrupting the services they provide. 
			  This can be achieved through techniques like rolling updates, where updates are applied to a subset of servers at a time, ensuring 
			  that the system remains available throughout the process. In a real-world scenario, a company might use rolling updates to apply 
			  security patches to their web servers without causing any downtime.
			  
# Auto-Healing
			- Auto-healing involves automatically detecting and resolving issues in a system. This can be done by setting up alerts and automated
              actions based on predefined conditions. For instance, a company might set up Prometheus to monitor the availability of their services 
			  and automatically restart any instances that become unavailable. This ensures that the system remains available and responsive at all times.
			  
##############################################################################################################################################################
## Agile   ########################################
###################################################

# What is Agile?
				- Agile is a conceptual framework and set of principles that guide an iterative, collaborative, and
				 adaptive approach to software development and project management. 
				 The key tenets of Agile include:
								1.Iterative Development :
											- Breaking down projects into smaller, manageable "sprints" that deliver working software incrementally
											- Allowing requirements and solutions to evolve through collaboration between self-organizing, cross-functional teams
								2.Adaptability
											- Embracing change and being responsive to customer needs, rather than following a rigid plan
											- Regularly reviewing progress and adjusting course as needed
								3.Collaboration
											- Emphasizing face-to-face communication and teamwork over rigid processes and documentation
											- Involving customers/stakeholders throughout the development lifecycle
								
								The Agile Manifesto, created in 2001, outlines the core values and principles that define the Agile philosophy:
											1. Individuals and interactions over processes and tools
											2. Working software over comprehensive documentation
											3. Customer collaboration over contract negotiation
											4. Responding to change over following a plan

# Agile Methodologies: Scrum and Kanban
				- Two of the most widely adopted Agile methodologies are Scrum and Kanban:
								1. Scrum
										- Structured framework with defined roles (Scrum Master, Product Owner, Development Team)
										- Work is completed in short, time-boxed iterations called Sprints
										- Daily Scrum meetings, Sprint Planning, Sprint Reviews, and Retrospectives
								
								2. Kanban
										- Focuses on visualizing workflow, limiting work-in-progress, and improving flow
										- Uses a Kanban board to track the status of work items as they move through the process
										- Emphasizes continuous delivery and evolutionary change
										
# What is Lean?
				- Lean is a management philosophy that originated in the Toyota Production System. It focuses on maximizing
 				  value for the customer while minimizing waste in the form of time, effort, and resources. 
				  The key principles of Lean include:
								1. Eliminating waste
								2. Amplifying learning
								3. Deciding as late as possible
								4. Delivering as fast as possible
								5. Empowering the team
								6. Building integrity in
								7. Seeing the whole
								
# Implementation of Lean
				- Implementing Lean involves identifying and eliminating non-value-adding activities, streamlining processes, 
				  and empowering teams to continuously improve. 
				Common Lean practices include:
								1. Value stream mapping to visualize and optimize workflows
								2. Just-in-time production to minimize inventory
								3. Kaizen (continuous improvement) events
								4. 5S (sort, set in order, shine, standardize, sustain) for workplace organization
								5. Poka-yoke (mistake-proofing) to prevent defects
	
# Lean and Agile in DevOps
				- Agile and Lean principles are foundational to the DevOps approach, which aims to bridge the gap between development 
				  and operations teams. 
				  Key ways Lean and Agile are applied in DevOps include:
								1. Continuous Integration and Continuous Deployment (CI/CD) to deliver software faster
								2. Automated testing and monitoring to ensure quality and reliability
								3. Collaborative, cross-functional teams to improve communication and alignment
								4. Experimentation and iterative improvement to respond to changing requirements
								
								

###############################################################################################################################
Introduction to Devops
		- DevOps is a software development method which focuses on communication, integration, and collaboration among IT professionals
		  to enable rapid deployment of products.
		- DevOps is a culture that promotes collaboration between Development and Operations Teams. This allows deploying code to production
          faster and in an automated way. It helps to increase an organization’s speed to deliver applications and services


DevOps Ecosystem
		- The DevOps ecosystem consists of various tools, practices, and cultural principles that enable faster and more reliable
		  software delivery:
		
		Practices :
				1. Continuous Integration (CI): Automatically building, testing, and integrating code changes into a shared repository.
				2. Continuous Delivery (CD): Automatically deploying software changes to production in a safe, reliable, and repeatable manner.
				3. Infrastructure as Code (IaC): Defining and managing infrastructure through code, enabling automated provisioning and configuration.
				4. Monitoring and Observability: Continuously monitoring application and infrastructure performance to quickly identify and resolve issues
		
		Tools : 
				1. Version Control (e.g. Git)
				2. Containerization (e.g. Docker)
				3. Orchestration (e.g. Kubernetes)
				4. Continuous Integration/Deployment Tools (e.g. Jenkins, CircleCI, Travis CI)
				5. Configuration Management (e.g. Ansible, Puppet, Chef)
				6. Monitoring and Observability Tools (e.g. Prometheus, Grafana, ELK Stack)
				
		- Cultural Principles :
				
				1. Collaboration: Breaking down silos between Dev and Ops teams to foster shared responsibility and accountability.
				2. Automation: Automating repetitive tasks to improve speed, consistency, and reliability.
				3. Continuous Improvement: Embracing an iterative, incremental, and experimental mindset to continuously improve processes.
				4. Blameless Culture: Fostering a culture where failures are seen as learning opportunities rather than individual faults.

		- DevOps Lifecycle :
				- The DevOps lifecycle consists of the following phases:
				1. Plan: Gather requirements, define the scope, and plan the software development process.
				2. Code: Write and version control the application code.
				3. Build: Compile the code and package it into a deployable artifact.
				4. Test: Automatically test the application to ensure it meets the required quality standards.
				5. Release: Deploy the application to the production environment.
				6. Deploy: Monitor the application in production and gather feedback.
				7. Operate: Maintain and support the application in production.
				8. Monitor: Continuously monitor the application and infrastructure performance.
				
		-CAMS Model :
				- The CAMS model is a framework that outlines the key principles of DevOps:
						
						1. Culture: Fostering a collaborative, transparent, and blameless culture.
						2. Automation: Automating repetitive tasks to improve speed and reliability.
						3. Measurement: Continuously measuring and monitoring the software delivery process.
						4. Sharing: Sharing knowledge, best practices, and learnings across the organization
						
		-Kaizen
				- Kaizen is the principle of continuous improvement, where teams constantly experiment, measure, and iterate
				  to enhance their processes and practices.
		
		-Immutable Deployment :
		
				- Immutable deployment is the practice of creating a new, fully configured environment for each deployment, 
				rather than modifying an existing environment. This ensures consistency and reliability across environments.
		
				
CI/CD Pipelines: Concepts and Real-World Practices
		- Continuous Integration (CI) and Continuous Deployment (CD)
					Continuous Integration (CI) and Continuous Deployment (CD) are two fundamental practices in modern software development 
					that work together to streamline the software delivery process.
		
		1. Continuous Integration (CI) is the process of regularly merging code changes from multiple developers into a shared repository, 
		   and then automatically building, testing, and validating the integrated codebase. This helps detect integration issues early and
		   ensures the codebase is always in a functional state.

		2. Continuous Deployment (CD) is the process of automatically deploying the validated code changes from the CI process directly to 
		   production environments. This enables faster and more frequent software releases, reducing the time between development and deployment.
		
CI/CD Pipelines :
		
		A CI/CD pipeline is an automated software development workflow that combines the practices of CI and CD. 
		It consists of a series of stages, such as source code management, building, testing, and deployment, that 
		are executed in a defined sequence to deliver software changes reliably and efficiently.
		
		- The key components of a CI/CD pipeline are:
					
					1. Source Code Management: A version control system, such as Git, that manages the codebase and tracks changes.
					2. Build: Automated processes that compile the source code and generate executable artifacts.
					3. Test: Automated testing frameworks that validate the functionality, performance, and security of the application.
					4. Deploy: Automated processes that deploy the verified artifacts to target environments, such as development, staging, and production.
					
IAM (Identity and Access Management): 
		
		- IAM is a crucial component in securing CI/CD pipelines. It ensures that only authorized users and systems can access and interact 
		  with the pipeline. This includes implementing role-based access control, managing user permissions, and integrating with identity 
		  providers for authentication.

Containerization (LXC, Docker): 
		
		- Containerization technologies, such as LXC (Linux Containers) and Docker, are widely used in CI/CD pipelines to create consistent,
		  isolated, and reproducible build and deployment environments. Containers ensure that the application and its dependencies are packaged 
		  together, making the deployment process more reliable and scalable.

Virtualization (KVM) :
    
        - Virtualization technologies, like KVM (Kernel-based Virtual Machine), can be used in CI/CD pipelines to provision and manage virtual
		  machines for testing, staging, and production environments. This allows for better resource utilization, isolation, and scalability
		  compared to physical hardware.
		  
#################################################################################################################################################

# Why Use Git?
			
		- Git is a powerful version control system that offers several key benefits for software development:
				
				1. Collaboration: Git enables multiple developers to work on the same project simultaneously, 
				   allowing them to merge changes seamlessly and resolve conflicts easily.
				2. History Tracking: Every change is recorded as a commit, allowing you to revert to previous 
				   versions of your code if needed. This provides a complete history of your project.
				3. Branching and Merging: Git's branching model allows you to create separate lines of development
    			   for new features or experiments without affecting the main codebase. Once the work is complete, 
				   the branch can be merged back into the main branch.
				4. Distributed Development: Each developer has a complete copy of the repository, including its history. 
				   This decentralization enhances collaboration and backup capabilities.
				5. Open Source Projects: Most open source projects use Git for version control. Learning Git allows you 
				   to contribute to these projects.
				6. Industry Standard: Git is widely used in the software industry, making it an essential skill for developers

	Core Concepts :
				
				1. Repositories: A repository (or repo) is a storage space where your project files and their history are kept. 
				   There are two types of repositories in Git: local repositories on your machine and remote repositories hosted 
				   on a server like GitHub.
				
				2. Commits: A commit is a snapshot of your project at a specific point in time. Each commit has a unique identifier (hash)
 				   and includes a message describing the changes made. Commits allow you to track and review the history of your project.

				3. Branches: A branch is a separate line of development. The default branch is called main or master. You can create new 
				   branches to work on features or fixes independently. Once the work is complete, the branch can be merged back into the 
				   main branch.
				
				4. Merging: Merging is the process of integrating changes from one branch into another. It allows you to combine the work 
				   done in different branches.
 
				5. Staging: Git marks modified files as "staged." Staging prepares changes for a snapshot you want to keep.
				
	
	Going Command Line : 
			To get started with Git from the command line, follow these steps:
				
				1. Initialize a Repository: Navigate to your project directory and initialize a Git repository using git init.
				2. Add Files: Add files to the staging area using " git add .".
				3. Commit Changes: Commit your changes with a descriptive message using " git commit -m "Initial commit" ".
				4. Check Status: Check the status of your repository using "git status".
				5. View Commit History: View the commit history using "git log".
				6. Create a Branch: Create a new branch using " git checkout -b feature/new-feature ".
				7. Switch Branches: Switch between branches using " git checkout main ".
				8. Merge Branches: Merge a branch into the main branch using "git merge feature/new-feature".
				9. Push to Remote: Push your changes to a remote repository using "git push origin main".
	

What is Jenkins?
		
		- Jenkins is an open source continuous integration (CI) server that helps automate the software development process. 
		  It manages and controls several stages of the software delivery pipeline, including build, test, and deployment.
		  
		- Jenkins follows a master-agent architecture. The Jenkins master server manages and coordinates the build process, 
		  while agents (or slaves) execute the actual builds. Agents can run on various platforms like Linux, Windows, or macOS.

		- Jenkins uses plugins to extend its functionality. There is a large ecosystem of plugins available for integrating with
          version control systems, build tools, testing frameworks, and deployment technologies.

Jenkins CI/CD Pipeline
		- Jenkins is commonly used to implement continuous integration and continuous delivery (CD) pipelines.
          Here's a typical Jenkins CI/CD workflow:
				
				1. Developer commits code changes to a version control system like Git.
				2. Jenkins detects the commit and triggers a build.
				3. Jenkins checks out the code and builds the application.
				4. Jenkins runs automated tests to verify the build.
				5. If tests pass, Jenkins deploys the application to a staging environment for further testing.
				6. Once approved, Jenkins deploys the application to production.

Introduction to AWS

		- AWS (Amazon Web Services) is a comprehensive cloud computing platform provided by Amazon that offers a wide range of
          on-demand services, including computing power, storage, databases, and more. AWS allows businesses and individuals to 
		  scale and manage their IT resources efficiently, without the need for extensive infrastructure investments.

Key AWS Services :
		
		1. EC2 (Elastic Compute Cloud): EC2 is the backbone of AWS, providing scalable and secure virtual servers (instances) 
		   that can be easily provisioned and managed. EC2 allows users to choose from a variety of instance types, operating 
		   systems, and software configurations to best suit their application needs.
		2. Lambda: AWS Lambda is a serverless computing service that allows you to run code without provisioning or managing 
		   servers. With Lambda, you can run your code in response to various events or triggers, such as HTTP requests, 
		   database updates, or scheduled events, without worrying about the underlying infrastructure.
		3. S3 (Simple Storage Service): S3 is a highly scalable and durable object storage service that can be used to store
           and retrieve any amount of data, from anywhere on the web. S3 provides features like versioning, encryption, and 
		   access control, making it a popular choice for storing and sharing files, backups, and other types of data.

Virtual Private Cloud (VPC) Setup
		
		- AWS VPC (Virtual Private Cloud) is a virtual network that allows you to launch AWS resources in a logically isolated
          virtual network. VPC enables you to have complete control over your virtual networking environment, including the 
		  selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways.

The key steps in setting up a VPC are:
		
		1. Create a VPC: Specify the IP address range for your VPC (e.g., 10.0.0.0/16).
		2. Create Subnets: Divide the VPC into smaller IP address ranges (subnets) for better organization and security.
		3. Configure Route Tables: Define the routing rules for the traffic within and outside the VPC.
		4. Set up Internet Gateway: Connect the VPC to the internet by creating an internet gateway.
		5. Configure Security Groups: Define firewall rules to control inbound and outbound traffic to your VPC resources.
		6. Launch Resources: Deploy your AWS resources, such as EC2 instances, within the VPC.
		
By setting up a VPC, you can have a fully customizable virtual network environment, allowing you to have complete control over your 
network configuration, security, and the placement of your AWS resources.




#######################################################################################################################################

Version Control Concepts and Benefits	:
			Version control systems are essential tools for managing changes to software projects over time. 
			They provide several key benefits:
					
					1. Single Source of Truth: Version control gives the entire team a centralized, shared repository that serves as 
					   the single source of truth for the project. This ensures everyone is working with the same codebase.
					2. Full History and Visibility: Version control systems maintain a complete audit trail of all changes made to files,
					   including who made the changes and when. This history allows teams to easily revert to previous versions if needed.
					3. Parallel Development: Version control enables multiple team members to work on the same project simultaneously,
					   merging their changes back into the main codebase.
					4. Collaboration and Productivity: By facilitating parallel development and providing change tracking, version control 
					   improves collaboration and boosts developer productivity.
					5. Rollback and Recovery: If an issue is introduced, version control makes it easy to identify the problematic change 
					   and roll back to a previous, working version of the codebase.
					   
Centralized vs. Distributed Version Control
			
			- There are two main types of version control systems:
			1. Centralized Version Control (e.g. Subversion): There is a single, central repository that all users access and 
			   commit changes to.
			2. Distributed Version Control (e.g. Git): Each user has their own local repository in addition to the central one.
               Users can commit changes to their local repo before pushing to the central one.
			3. Distributed version control systems like Git are more modern, flexible, and popular today. They offer advantages 
			   like faster performance, better offline capabilities, and more robust branching and merging features

Version Control Workflow
		    - The typical version control workflow involves the following steps:
			
			1. Clone/Checkout: Get a local copy of the project's codebase from the central repository.
			2. Edit: Make changes to files in your local working copy.
			3. Stage: Select the changes you want to commit.
			4. Commit: Record the staged changes in your local repository.
			5. Push: Upload your local commits to the central repository so others can access them.
			6. Pull/Update: Retrieve the latest changes from the central repository and merge them into your local working copy.
			

Infrastructure as Code (IaC)
What is Infrastructure as Code?
	
			- Infrastructure as Code (IaC) is the practice of managing and provisioning computing infrastructure (e.g. servers, networks, load balancers)
			  through machine-readable definition files, rather than manual processes. These definition files are version controlled, just like application 
			  source code.
			  
IaC Tools and Frameworks : 
			- Some popular IaC tools and frameworks include:
			
			1. Terraform: A declarative IaC tool that supports multiple cloud providers.
			2. AWS CloudFormation: Amazon's proprietary IaC service for managing AWS resources.
			3. Ansible: A configuration management tool that uses YAML-based playbooks to define infrastructure.
			4. Puppet: A declarative IaC tool that focuses on configuration management.
			5. Chef: A framework for automating server configuration and application deployment.
			
What is Docker?
Docker is an open platform for developing, shipping, and running applications. 

Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. 

By taking advantage of Dockerâ€™s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.


Docker Platform
Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.

Docker provides tooling and a platform to manage the lifecycle of your containers:

Develop your application and its supporting components using containers.
The container becomes the unit for distributing and testing your application.
		- When you are ready, deploy your application into your production environment, as a container or an orchestrated service. 
		   This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.


Docker Swarm

Docker swarm is a container orchestration tool, meaning that it allows the user to manage multiple containers deployed across multiple host machines. 
One of the key benefits associated with the operation of a docker swarm is the high level of availability offered for applications.



Docker Swarm is an open-source container orchestration platform built and maintained by Docker. Under the hood, Docker Swarm converts multiple 
Docker instances into a single virtual host. A Docker Swarm cluster generally contains three items:

1. Nodes
2. Services and tasks
3. Load balancers


Nodes are individual instances of the Docker engine that control your cluster and manage the containers used to run your services and tasks.
Docker Swarm clusters also include load balancing to route requests across nodes.

Kubernetes

Kubernetes is an open source container orchestration platform that was initially designed by Google to manage their containers.
Kubernetes has a more complex cluster structure than Docker Swarm. It usually has a builder and worker nodes architecture divided 
further into pods, namespaces, config maps, and so on.


Microservice Deployment

Containerization and container orchestration are particularly well-suited for deploying and managing microservice-based applications.
Microservices are a software architecture pattern where a large application is composed of small, independent, and loosely coupled services.